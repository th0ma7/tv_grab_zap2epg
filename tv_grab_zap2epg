#!/bin/bash
# tv_grab_zap2epg gracenote.com TV schedule grabber for tvheadend
################################################################################
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
################################################################################

#
DaysPattern='^[1-9]$|^1[0-4]$'
CACodePattern='^[A-Z][0-9][A-Z][ ]?[0-9][A-Z][0-9]$'
USCodePattern='^[0-9]{5}$'
#
VERSION="4.0"
Quiet="false"
Debug="false"
ZipCode=""
Offset=""
Days=""

# Function to detect system type
detect_system() {
    # Check if it's a Raspberry Pi
    if [ -f /proc/device-tree/model ] && grep -qi "raspberry" /proc/device-tree/model 2>/dev/null; then
        echo "raspberry"
    elif [ -f /proc/cpuinfo ] && grep -qi "raspberry" /proc/cpuinfo 2>/dev/null; then
        echo "raspberry"
    # Check if it's Synology
    elif [ "$(uname -a | grep -i synology)" ]; then
        echo "synology"
    else
        echo "linux"
    fi
}

# Manage user arguments
while [ $# -gt 0 ]; do
   case "$1" in
      -d | --description )
        printf "North America (tvlistings.gracenote.com using zap2epg)\n"
        exit 0
        ;;

      -v | --version )
        printf "%s\n" "$VERSION"
        exit 0
        ;;

      -c | --capabilities )
        printf "baseline\n"
        exit 0
        ;;

      -q | --quiet )
        Quiet="true"
        ;;

      --debug )
        Debug="true"
        ;;

      -o | --output )
        shift
        XMLTV="$1"
        ;;

      -d | --days )
        shift
        Days="$1"
        ;;

      --offset )
        shift
        Offset="$1"
        ;;

      --config-file )
        shift
        ConfFile="$1"
        ;;

      --basedir )
        shift
        BaseDir="$1"
        ;;

      --zip | --postal | --code )
        shift
        # Remove " " from postal code if any
        ZipCode=$(echo ${1} | sed 's/ //g')
        ;;

      -* )
        printf "unknown option: %s\n" "$1"
        printf "Usage: %s: [--description] [--version] [--capabilities] [--quiet] [--debug] [--basedir DIR]\n" ${0##*/}
        exit 2
        ;;
   esac
   shift
done

# Intelligent BaseDir detection if not specified via --basedir
if [ ! "${BaseDir}" ]; then
   SYSTEM_TYPE=$(detect_system)

   case "$SYSTEM_TYPE" in
       "raspberry")
           # Raspberry Pi - use Kodi path if available, otherwise default
           if [ -d "$HOME/script.module.zap2epg/epggrab" ]; then
               BaseDir="$HOME/script.module.zap2epg/epggrab"
           else
               BaseDir="$HOME/zap2epg"
           fi
           ;;
       "synology")
           # Synology NAS - enhanced logic with zap2epg subdirectory
           export PATH=/var/packages/tvheadend/target/env/bin:${PATH}
           if [ $(uname -a | sed -e 's/.* #//' -e 's/ .*//') -lt 40000 ]; then
               # DSM6
               BaseDir="/var/packages/tvheadend/target/var/epggrab/zap2epg"
           else
               # DSM7
               BaseDir="/var/packages/tvheadend/var/epggrab/zap2epg"
           fi
           ;;
       *)
           # Standard Linux - check for hts user, otherwise use $HOME/zap2epg
           if [ "$(id hts 2>/dev/null)" ]; then
               BaseDir="$HOME/zap2epg"
           else
               BaseDir="$HOME/zap2epg"
           fi
           ;;
   esac
fi

# Define directory structure based on BaseDir
CacheDir="$BaseDir/cache"
ConfDir="$BaseDir/conf"
ConfFile="$ConfDir/zap2epg.xml"
LogDir="$BaseDir/log"
LogFile="$LogDir/zap2epg.log"
XMLTV="$CacheDir/xmltv.xml"

zap2epg() {
	read -r -d '' script <<-"----EOF"
import urllib.request, urllib.error, urllib.parse
import base64
import codecs
import time
import datetime
import _strptime
import calendar
import gzip
import os
import logging
import re
import json
import sys
from os.path import dirname
import xml.etree.ElementTree as ET
from collections import OrderedDict
import hashlib
import requests
from requests.auth import HTTPDigestAuth

def mainRun(userdata):
    settingsFile = os.path.join(userdata, os.environ.get('ConfFile'))
    logging.info('Reading configuration from: %s', settingsFile)

    try:
        settings = ET.parse(settingsFile)
        root = settings.getroot()
    except Exception as e:
        logging.error('Cannot parse configuration file %s: %s', settingsFile, e)
        return None, None, None

    settingsDict = {}
    validSettings = {}  # Track valid settings for config cleanup
    deprecatedSettings = []  # Track deprecated settings found

    # Get version from environment variable
    script_version = os.environ.get('VERSION', '4.0')
    logging.info('Running zap2epg-%s with intelligent configuration management', script_version)
    kodiVersion = root.attrib.get('version')
    logging.info('Configuration version: %s', kodiVersion)

    for setting in root.findall('setting'):
        settingID = setting.get('id')
        if kodiVersion == '2':
            settingStr = setting.text
        else:
            # For version 3, try the 'value' attribute first, then text
            settingStr = setting.get('value')
            if settingStr is None:
                settingStr = setting.text
            if settingStr == '':
                settingStr = None
        settingsDict[settingID] = settingStr
        logging.debug('Config setting: %s = %s (from %s)', settingID, settingStr, 'attribute' if setting.get('value') is not None else 'text')

    # Initialize variables with default values to prevent None concatenation
    stationList = None
    zipcode = None
    lineup = None
    lineupcode = None
    device = None
    days = None
    redays = None
    xdetails = False
    xdesc = False
    epicon = None
    epgenre = None
    tvhoff = False
    tvhurl = None
    tvhport = None
    usern = None
    passw = None
    chmatch = False
    tvhmatch = False
    stitle = False

    # Helper function to parse boolean values
    def parse_boolean(value):
        if value is None:
            return False
        if isinstance(value, bool):
            return value
        if isinstance(value, str):
            return value.lower() in ('true', '1', 'yes', 'on')
        return bool(value)

    # Process settings and track valid ones
    for setting in settingsDict:
        if setting == 'slist':
            stationList = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'zipcode':
            if "ZipCode" in os.environ:
                zipcode = os.environ.get('ZipCode')
                logging.info('Using zipcode from environment: %s', zipcode)
            else:
                zipcode = settingsDict[setting]
                logging.info('Using zipcode from config: %s', zipcode)
            validSettings[setting] = settingsDict[setting]
        elif setting == 'lineup':
            lineup = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'lineupcode':
            lineupcode = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'device':
            device = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'days':
            if "Days" in os.environ:
                days = os.environ.get('Days')
            else:
                days = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'redays':
            redays = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'xdetails':
            xdetails = parse_boolean(settingsDict[setting])
            logging.debug('Parsed xdetails: %s -> %s', settingsDict[setting], xdetails)
            validSettings[setting] = settingsDict[setting]
        elif setting == 'xdesc':
            xdesc = parse_boolean(settingsDict[setting])
            logging.debug('Parsed xdesc: %s -> %s', settingsDict[setting], xdesc)
            validSettings[setting] = settingsDict[setting]
        elif setting == 'epicon':
            epicon = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'epgenre':
            epgenre = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'tvhoff':
            tvhoff = parse_boolean(settingsDict[setting])
            logging.debug('Parsed tvhoff: %s -> %s', settingsDict[setting], tvhoff)
            validSettings[setting] = settingsDict[setting]
        elif setting == 'tvhurl':
            tvhurl = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'tvhport':
            tvhport = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'usern':
            usern = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'passw':
            passw = settingsDict[setting]
            validSettings[setting] = settingsDict[setting]
        elif setting == 'chmatch':
            chmatch = parse_boolean(settingsDict[setting])
            logging.debug('Parsed chmatch: %s -> %s', settingsDict[setting], chmatch)
            validSettings[setting] = settingsDict[setting]
        elif setting == 'tvhmatch':
            tvhmatch = parse_boolean(settingsDict[setting])
            logging.debug('Parsed tvhmatch: %s -> %s', settingsDict[setting], tvhmatch)
            validSettings[setting] = settingsDict[setting]
        elif setting == 'stitle':
            stitle = parse_boolean(settingsDict[setting])
            logging.debug('Parsed stitle: %s -> %s', settingsDict[setting], stitle)
            validSettings[setting] = settingsDict[setting]
        elif setting.startswith('desc'):
            # Handle deprecated desc01-desc20 complex formatting system
            if re.match(r'desc[0-9]{2}', setting):
                deprecatedSettings.append(f'{setting} (deprecated custom formatting)')
                logging.info('Found deprecated custom formatting setting: %s (will be removed)', setting)
            else:
                logging.warning('Unknown description setting: %s (will be removed)', setting)
                deprecatedSettings.append(f'{setting} (unknown desc setting)')
        elif setting == 'useragent':
            # Handle deprecated useragent setting
            deprecatedSettings.append(f'{setting} (deprecated)')
            logging.info('Found deprecated setting: %s (will be removed)', setting)
        else:
            # Unknown setting - could be future setting or typo
            logging.warning('Unknown configuration setting: %s = %s (will be removed)', setting, settingsDict[setting])
            deprecatedSettings.append(f'{setting} (unknown)')

    def cleanConfigFile(configFile, validSettings, version, deprecatedList):
        """Intelligently clean configuration file by removing deprecated settings"""
        try:
            # Only proceed if there are deprecated settings to remove
            if not deprecatedList:
                logging.debug('No deprecated settings found - configuration file is clean')
                return

            # Create backup only when modifications are needed
            import shutil
            backup_file = f"{configFile}.backup.{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
            shutil.copy2(configFile, backup_file)
            logging.info('Created configuration backup: %s', backup_file)

            # Build new XML structure
            from xml.dom import minidom

            # Create new document
            doc = minidom.Document()
            settings_elem = doc.createElement('settings')
            settings_elem.setAttribute('version', version or '3')
            doc.appendChild(settings_elem)

            # Add valid settings in a logical order
            setting_order = [
                'zipcode', 'lineupcode', 'lineup', 'device', 'days', 'redays',
                'slist', 'stitle', 'xdetails', 'xdesc', 'epgenre', 'epicon',
                'tvhoff', 'usern', 'passw', 'tvhurl', 'tvhport', 'tvhmatch', 'chmatch'
            ]

            # Add settings in preferred order (using original format with text content)
            for setting_id in setting_order:
                if setting_id in validSettings:
                    setting_elem = doc.createElement('setting')
                    setting_elem.setAttribute('id', setting_id)
                    if validSettings[setting_id] is not None:
                        # Use text content instead of value attribute (original format)
                        text_node = doc.createTextNode(str(validSettings[setting_id]))
                        setting_elem.appendChild(text_node)
                    settings_elem.appendChild(setting_elem)

            # Add any remaining valid settings not in the ordered list
            for setting_id, value in validSettings.items():
                if setting_id not in setting_order:
                    setting_elem = doc.createElement('setting')
                    setting_elem.setAttribute('id', setting_id)
                    if value is not None:
                        # Use text content instead of value attribute (original format)
                        text_node = doc.createTextNode(str(value))
                        setting_elem.appendChild(text_node)
                    settings_elem.appendChild(setting_elem)

            # Write cleaned configuration (with proper XML format but original element structure)
            with open(configFile, 'w', encoding='utf-8') as f:
                # Write XML declaration
                f.write('<?xml version="1.0" encoding="utf-8"?>\n')
                # Write settings element with proper formatting
                f.write(f'<settings version="{version or "3"}">\n')

                # Write settings in order with original format (text content, not attributes)
                for setting_id in setting_order:
                    if setting_id in validSettings:
                        value = validSettings[setting_id]
                        if value is not None and str(value).strip():
                            f.write(f'  <setting id="{setting_id}">{value}</setting>\n')
                        else:
                            f.write(f'  <setting id="{setting_id}"></setting>\n')

                # Write any remaining settings not in ordered list
                for setting_id, value in validSettings.items():
                    if setting_id not in setting_order:
                        if value is not None and str(value).strip():
                            f.write(f'  <setting id="{setting_id}">{value}</setting>\n')
                        else:
                            f.write(f'  <setting id="{setting_id}"></setting>\n')

                f.write('</settings>\n')

            logging.info('Configuration cleaned successfully')
            logging.info('  Removed %d deprecated/unknown settings: %s',
                        len(deprecatedList), ', '.join(deprecatedList))
            logging.info('  Kept %d valid settings', len(validSettings))

        except Exception as e:
            logging.error('Error cleaning configuration file: %s', str(e))
            logging.error('Continuing with existing configuration...')

    # Clean configuration file if deprecated settings were found
    if deprecatedSettings:
        cleanConfigFile(settingsFile, validSettings, kodiVersion, deprecatedSettings)

    # Debug: Display all values read from configuration
    logging.info('Configuration values processed:')
    logging.info('  zipcode: %s', zipcode)
    logging.info('  lineup: %s', lineup)
    logging.info('  xdetails (download extended data): %s', xdetails)
    logging.info('  xdesc (use extended descriptions): %s', xdesc)

    # Validate required variables and set defaults
    if not zipcode:
        logging.error('Zipcode is required but not found in configuration')
        logging.error('Available settings in config: %s', list(settingsDict.keys()))
        # Display configuration file content for debugging
        try:
            with open(settingsFile, 'r') as f:
                config_content = f.read()
                logging.error('Configuration file content: %s', config_content)
        except Exception as e:
            logging.error('Cannot read config file: %s', e)
        return None, None, None
    if not lineupcode:
        lineupcode = 'lineupId'  # Default value
    if not device:
        device = '-'  # Default value
    if not days:
        days = '1'  # Default value

    # Determine if we need to download extended details (xdetails OR xdesc)
    need_extended_download = xdetails or xdesc

    # Log configuration logic clearly
    if xdesc and not xdetails:
        logging.info('xdesc=true detected - automatically enabling extended details download')
    elif xdetails and not xdesc:
        logging.info('xdetails=true - downloading extended data but using basic descriptions')
    elif xdetails and xdesc:
        logging.info('Both xdetails and xdesc enabled - full extended functionality')
    else:
        logging.info('Extended features disabled - using basic guide data only')

    if lineupcode != 'lineupId':
        chmatch = False
        tvhmatch = False
    if zipcode.isdigit():
        country = 'USA'
        logging.info('\tCountry: United States of America [%s]', country)
        logging.info('\tZIP code: %s', zipcode)
    else:
        country = 'CAN'
        logging.info('\tCountry: Canada [%s]', country)
        logging.info('\tPostal code: %s', zipcode)
    if "Offset" in os.environ:
        offset = float(os.environ.get('Offset'))
    else:
        offset = 0.0
    pythonStartTime = time.time()
    dayHours = int(days) * 8
    gridtimeStart = (int(time.mktime(time.strptime(str(datetime.datetime.now().replace(microsecond=0,second=0,minute=0)), '%Y-%m-%d %H:%M:%S'))) + offset*86400)
    cacheDir = os.path.join(userdata, os.environ.get('CacheDir'))
    schedule = {}
    tvhMatchDict = {}

    # Debug: Log description parameters
    logging.info('\tExtended details download: %s (triggered by xdetails=%s or xdesc=%s)',
                 need_extended_download, xdetails, xdesc)
    logging.info('\tExtended descriptions: %s', xdesc)
    logging.info('\tIntelligent description formatting: enabled (no complex configuration required)')

    logging.info('\tTV Guide duration: %s days', days)
    if int(offset) > 1:
        logging.info('\tTV Guide Start: %i [offset: %i days]', int(gridtimeStart), int(offset))
    elif int(offset) == 1:
        logging.info('\tTV Guide Start: %i [offset: %i day]', int(gridtimeStart), int(offset))
    else:
        logging.info('\tTV Guide Start: %i', int(gridtimeStart))
    logging.info('\tLineup: %s', lineup)
    logging.info('\tConfiguration file: %s', settingsFile)
    logging.info('\tCaching directory path: %s', cacheDir)

    # =========================================================================
    # SHARED CLASSES AND FUNCTIONS FOR OPTIMIZED DOWNLOADS
    # =========================================================================

    class OptimizedDownloader:
        """Optimized download manager with WAF protection"""

        def __init__(self, base_delay=1.0, min_delay=0.5):
            self.session = None
            self.user_agents = [
                'Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'
            ]
            self.last_request_time = 0
            self.base_delay = base_delay
            self.min_delay = min_delay
            self.current_delay = base_delay
            self.consecutive_failures = 0
            self.waf_blocks = 0
            self.total_requests = 0
            self.current_ua_index = 0

        def init_session(self):
            """Initialize optimized session with forced connection reuse"""
            if self.session:
                self.session.close()

            self.session = requests.Session()

            # Realistic headers with forced Keep-Alive
            base_headers = {
                'Accept': 'application/json, text/html, application/xhtml+xml, */*',
                'Accept-Language': 'en-US,en;q=0.9,fr;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Keep-Alive': 'timeout=60, max=100',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
            self.session.headers.update(base_headers)

            # Optimized configuration for connection reuse
            from requests.adapters import HTTPAdapter
            from urllib3.util.retry import Retry

            retry_strategy = Retry(
                total=0,
                backoff_factor=0,
                status_forcelist=[]  # Don't auto-retry
            )

            # Optimized adapter with minimal connection pool
            adapter = HTTPAdapter(
                pool_connections=1,    # Single connection pool
                pool_maxsize=1,        # Single connection in pool
                max_retries=retry_strategy,
                pool_block=True        # Block if pool full to force reuse
            )

            self.session.mount('https://', adapter)
            self.session.mount('http://', adapter)

            # Configuration urllib3 for persistence
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

            # Set initial User-Agent
            self.rotate_user_agent()
            logging.info('Optimized session initialized with persistent connections')
            logging.debug('  Connection pooling: 1 connection max, keep-alive enabled')

        def rotate_user_agent(self):
            """Rotate User-Agent intelligently"""
            self.current_ua_index = (self.current_ua_index + 1) % len(self.user_agents)
            new_ua = self.user_agents[self.current_ua_index]
            self.session.headers.update({'User-Agent': new_ua})
            logging.debug('  User-Agent rotated: %s', new_ua[:50] + '...')

        def adaptive_delay(self):
            """Apply adaptive delay between requests"""
            import random

            # Calculate adaptive delay
            if self.consecutive_failures > 2:
                self.current_delay = min(self.base_delay * (1.5 ** self.consecutive_failures), 15.0)
            elif self.consecutive_failures == 0:
                self.current_delay = max(self.min_delay, self.current_delay * 0.95)

            # Add random variation
            delay = self.current_delay + random.uniform(-0.2, 0.5)
            delay = max(self.min_delay, delay)

            # Respect delay since last request
            elapsed = time.time() - self.last_request_time
            if elapsed < delay:
                sleep_time = delay - elapsed
                logging.debug('  Adaptive delay: %.2fs (failures: %d)', sleep_time, self.consecutive_failures)
                time.sleep(sleep_time)

            self.last_request_time = time.time()

        def is_waf_blocked(self, response_text):
            """Detect WAF blocking"""
            waf_indicators = [
                'Human Verification', 'captcha-container', 'AwsWafIntegration',
                '403 Forbidden', 'Access Denied', 'challenge.js'
            ]
            return any(indicator in response_text for indicator in waf_indicators)

        def handle_waf_block(self, extra_delay_range=(3, 8)):
            """Handle WAF blocking with appropriate backoff"""
            import random
            self.waf_blocks += 1
            self.consecutive_failures += 1
            extra_delay = random.uniform(*extra_delay_range)
            logging.warning('  WAF block detected, backing off %.1fs...', extra_delay)
            time.sleep(extra_delay)
            if self.total_requests % 10 == 0:  # Rotate occasionally after blocks
                self.rotate_user_agent()

        def download_with_retry_urllib(self, url, data=None, max_retries=3, timeout=None):
            """Download using urllib (like original version) with intelligent retry and WAF handling"""
            self.total_requests += 1

            # Adaptive timeouts based on history
            if timeout is None:
                if self.consecutive_failures == 0:
                    timeout = 6  # Fast if everything is OK
                elif self.consecutive_failures == 1:
                    timeout = 10  # Medium after 1 failure
                else:
                    timeout = 15  # Longer if repeated problems

            # Periodic User-Agent rotation
            if self.total_requests % 25 == 0:
                self.rotate_user_agent()

            for attempt in range(max_retries):
                self.adaptive_delay()

                current_timeout = timeout + (attempt * 2)  # Increase timeout on each retry
                current_ua = self.user_agents[self.current_ua_index]

                # Build display URL with parameters
                if data:
                    display_url = f'{url}?{data.decode("utf-8") if isinstance(data, bytes) else data}'
                else:
                    display_url = url

                logging.debug('  Attempt %d/%d: %s (timeout: %ds)',
                            attempt + 1, max_retries,
                            display_url[:100] + '...' if len(display_url) > 100 else display_url,
                            current_timeout)

                try:
                    # Use urllib exactly like the original working version
                    import urllib.request
                    URLcontent = urllib.request.Request(url, data=data, headers={'User-Agent': current_ua})
                    JSONcontent = urllib.request.urlopen(URLcontent, timeout=current_timeout).read()

                    if JSONcontent and len(JSONcontent) > 10:
                        # Check that it's valid JSON
                        try:
                            json.loads(JSONcontent)
                            self.consecutive_failures = max(0, self.consecutive_failures - 1)
                            logging.debug('  Success: %d bytes received', len(JSONcontent))
                            return JSONcontent
                        except json.JSONDecodeError:
                            logging.warning('  Invalid JSON received on attempt %d', attempt + 1)
                            self.consecutive_failures += 1
                    else:
                        logging.warning('  Empty/small response on attempt %d: %d bytes',
                                      attempt + 1, len(JSONcontent) if JSONcontent else 0)
                        self.consecutive_failures += 1

                except urllib.error.HTTPError as e:
                    if e.code == 403:
                        self.handle_waf_block()
                        continue
                    logging.warning('  HTTP Error %d on attempt %d: %s', e.code, attempt + 1, e.reason)
                    if e.code in [404, 410]:
                        break  # Don't retry for permanent errors
                    self.consecutive_failures += 1

                except urllib.error.URLError as e:
                    logging.warning('  URL Error on attempt %d: %s', attempt + 1, str(e.reason))
                    self.consecutive_failures += 1

                except Exception as e:
                    logging.warning('  Unexpected error on attempt %d: %s', attempt + 1, str(e))
                    self.consecutive_failures += 1

                # Wait before retry
                if attempt < max_retries - 1:
                    import random
                    retry_delay = random.uniform(1, 3)
                    time.sleep(retry_delay)

            # All retries failed
            logging.warning('  All %d attempts failed', max_retries)
            return None

        def download_with_retry(self, url, method='GET', data=None, max_retries=3, timeout=None):
            """Download with intelligent retry, adaptive timeouts and WAF handling (for guide)"""
            self.total_requests += 1

            # Adaptive timeouts based on history
            if timeout is None:
                if self.consecutive_failures == 0:
                    timeout = 6  # Fast if everything is OK
                elif self.consecutive_failures == 1:
                    timeout = 10  # Medium after 1 failure
                else:
                    timeout = 15  # Longer if repeated problems

            # Periodic User-Agent rotation
            if self.total_requests % 25 == 0:
                self.rotate_user_agent()

            for attempt in range(max_retries):
                self.adaptive_delay()

                current_timeout = timeout + (attempt * 2)  # Increase timeout on each retry

                # Build display URL with parameters for POST
                if method.upper() == 'POST' and data:
                    display_url = f'{url}?{data}'
                else:
                    display_url = url

                logging.debug('  Attempt %d/%d: %s (timeout: %ds)',
                            attempt + 1, max_retries,
                            display_url[:100] + '...' if len(display_url) > 100 else display_url,
                            current_timeout)

                try:
                    if method.upper() == 'POST':
                        response = self.session.post(url, data=data, timeout=current_timeout, allow_redirects=False)
                    else:
                        response = self.session.get(url, timeout=current_timeout, allow_redirects=False)

                    # Check WAF blocking
                    if response.status_code == 403:
                        self.handle_waf_block()
                        continue

                    if self.is_waf_blocked(response.text):
                        self.handle_waf_block((5, 12))  # Longer delay for CAPTCHA
                        continue

                    # Check response status
                    if response.status_code == 200:
                        self.consecutive_failures = max(0, self.consecutive_failures - 1)
                        logging.debug('  Success: %d bytes received', len(response.content))
                        return response.content
                    else:
                        logging.warning('  HTTP %d received', response.status_code)
                        if response.status_code in [404, 410]:
                            break  # Don't retry for permanent errors
                        self.consecutive_failures += 1

                except requests.exceptions.Timeout:
                    logging.warning('  Timeout (%ds) on attempt %d', current_timeout, attempt + 1)
                    self.consecutive_failures += 1

                except requests.exceptions.ConnectionError as e:
                    logging.warning('  Connection error on attempt %d: %s', attempt + 1, str(e))
                    self.consecutive_failures += 1
                    # Force reconnection on connection errors
                    self.session.close()
                    self.init_session()

                except requests.exceptions.RequestException as e:
                    logging.warning('  Request error on attempt %d: %s', attempt + 1, str(e))
                    self.consecutive_failures += 1

                # Wait before retry
                if attempt < max_retries - 1:
                    import random
                    retry_delay = random.uniform(1, 3)
                    time.sleep(retry_delay)

            # All retries failed
            logging.warning('  All %d attempts failed', max_retries)
            return None

        def close(self):
            """Clean shutdown"""
            if self.session:
                self.session.close()
                self.session = None

        def get_stats(self):
            """Get download statistics"""
            return {
                'total_requests': self.total_requests,
                'waf_blocks': self.waf_blocks,
                'consecutive_failures': self.consecutive_failures,
                'current_delay': self.current_delay
            }

    # Create shared downloader instance
    downloader = OptimizedDownloader(base_delay=0.8, min_delay=0.4)

    # Configure urllib3 logging to reduce verbosity
    import urllib3
    urllib3.disable_warnings()
    import logging as urllib_logging
    urllib_logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)

    downloader.init_session()

    def tvhMatchGet():
        try:
            tvhUrlBase = 'http://' + tvhurl + ":" + tvhport
            channels_url = tvhUrlBase + '/api/channel/grid?all=1&limit=999999999&sort=name&filter=[{"type":"boolean","value":true,"field":"enabled"}]'
            if usern is not None and passw is not None:
                logging.info('Tvheadend access using username and password...')
                response = requests.get(channels_url, auth=HTTPDigestAuth(usern, passw), timeout=5)
            else:
                logging.info('Tvheadend anonymous access...')
                response = requests.get(channels_url, timeout=5)
            if response.status_code == 200:
                logging.info('Accessing Tvheadend channel list from: %s', tvhUrlBase)
                channels = response.json()
                for ch in channels['entries']:
                    channelName = ch['name']
                    channelNum = ch['number']
                    tvhMatchDict[channelNum] = channelName
                logging.info('%s Tvheadend channels found...', str(len(tvhMatchDict)))
            else:
                logging.warning('Tvheadend returned status code: %s', response.status_code)
        except Exception as e:
            logging.warning('Cannot connect to Tvheadend: %s', str(e))
            logging.info('Continuing without Tvheadend channel matching...')
            pass

    def deleteOldCache(gridtimeStart):
        logging.info('Checking for old cache files...')
        try:
            if os.path.exists(cacheDir):
                entries = os.listdir(cacheDir)
                for entry in entries:
                    oldfile = entry.split('.')[0]
                    if oldfile.isdigit():
                        fn = os.path.join(cacheDir, entry)
                        if (int(oldfile)) < (gridtimeStart + (int(redays) * 86400)):
                            try:
                                os.remove(fn)
                                logging.info('Deleting old cache: %s', entry)
                            except OSError as e:
                                logging.warning('Error Deleting: %s - %s.' % (e.filename, e.strerror))
        except Exception as e:
            logging.exception('Exception: deleteOldCache - %s', e.strerror)

    def deleteOldShowCache(showList):
        logging.info('Checking for old show cache files...')
        try:
            if os.path.exists(cacheDir):
                entries = os.listdir(cacheDir)
                for entry in entries:
                    oldfile = entry.split('.')[0]
                    if not oldfile.isdigit():
                        fn = os.path.join(cacheDir, entry)
                        if oldfile not in showList:
                            try:
                                os.remove(fn)
                                logging.info('Deleting old show cache: %s', entry)
                            except OSError as e:
                                logging.warning('Error Deleting: %s - %s.' % (e.filename, e.strerror))
        except Exception as e:
            logging.exception('Exception: deleteOldshowCache - %s', e.strerror)

    def convTime(t):
        return time.strftime("%Y%m%d%H%M%S",time.localtime(int(t)))

    def convHTML(data):
        if data is None:
            return ""
        data = str(data)
        data = data.replace('&','&amp;')
        data = data.replace('"','&quot;')
        data = data.replace("'",'&apos;')
        data = data.replace('<','&lt;')
        data = data.replace('>','&gt;')
        return data;

    def convTitleExcept(data):
        exception = "CTV CW HD ION ION: NHK PBS TV TVA"
        data = " ".join([word.title() if word not in exception else word for word in data.split(" ")])
        return data;

    def savepage(fn, data):
        if not os.path.exists(cacheDir):
            os.mkdir(cacheDir)
        fileDir = os.path.join(cacheDir, fn)
        with gzip.open(fileDir,"wb+") as f:
            f.write(data)
            f.close()

    def genreSort(EPfilter, EPgenre):
        genreList = []
        if epgenre == '2':
            for g in EPgenre:
                if g != "Comedy":
                    genreList.append(g)
            if 'Movie' in genreList or 'movie' in genreList or 'Movies' in genreList:
                genreList.insert(0, "Movie / Drama")
            if 'News' in genreList:
                genreList.insert(0, "News / Current affairs")
            if 'Game show' in genreList:
                genreList.insert(0, "Game show / Quiz / Contest")
            if 'Law' in genreList:
                genreList.insert(0, "Show / Game show")
            if 'Art' in genreList or 'Culture' in genreList:
                genreList.insert(0, "Arts / Culture (without music)")
            if 'Entertainment' in genreList:
                genreList.insert(0, "Popular culture / Traditional Arts")
            if 'Politics' in genreList or 'Social' in genreList or 'Public affairs' in genreList:
                genreList.insert(0, "Social / Political issues / Economics")
            if 'Education' in genreList or 'Science' in genreList:
                genreList.insert(0, "Education / Science / Factual topics")
            if 'How-to' in genreList:
                genreList.insert(0, "Leisure hobbies")
            if 'Travel' in genreList:
                genreList.insert(0, "Tourism / Travel")
            if 'Sitcom' in genreList:
                genreList.insert(0, "Variety show")
            if 'Talk' in genreList:
                genreList.insert(0, "Talk show")
            if 'Children' in genreList:
                genreList.insert(0, "Children's / Youth programs")
            if 'Animated' in genreList:
                genreList.insert(0, "Cartoons / Puppets")
            if 'Music' in genreList:
                genreList.insert(0, "Music / Ballet / Dance")
        if epgenre == '1':
            for g in EPgenre:
                genreList.append(g)
            if 'Movie' in genreList or 'movie' in genreList or 'Movies' in genreList:
                genreList = ["Movie / Drama"]
            elif 'News' in genreList:
                genreList = ["News / Current affairs"]
            elif 'News magazine' in genreList:
                genreList = ["News magazine"]
            elif 'Public affairs' in genreList:
                genreList = ["News / Current affairs"]
            elif 'Interview' in genreList:
                genreList = ["Discussion / Interview / Debate"]
            elif 'Game show' in genreList:
                genreList = ["Game show / Quiz / Contest"]
            elif 'Talk' in genreList:
                genreList = ["Talk show"]
            elif 'Sports' in genreList:
                genreList = ["Sports"]
            elif 'Sitcom' in genreList:
                genreList = ["Variety show"]
            elif 'Children' in genreList:
                genreList = ["Children's / Youth programs"]
            else:
                genreList = ["Variety show"]
        if epgenre == '3':
            for g in EPgenre:
                genreList.append(g)
        if 'Movie' in genreList:
            genreList.remove('Movie')
            genreList.insert(0, 'Movie')
        return genreList

    def printHeader(fh, enc):
        logging.info('Creating xmltv.xml file...')
        fh.write("<?xml version=\"1.0\" encoding=\""+ enc + "\"?>\n")
        fh.write("<!DOCTYPE tv SYSTEM \"xmltv.dtd\">\n\n")
        fh.write("<tv source-info-url=\"http://tvschedule.gracenote.com/\" source-info-name=\"gracenote.com\">\n")

    def printFooter(fh):
        fh.write("</tv>\n")

    def printStations(fh):
        global stationCount
        stationCount = 0
        try:
            logging.info('Writing Stations to xmltv.xml file...')
            try:
                scheduleSort = OrderedDict(sorted(iter(schedule.items()), key=lambda x: int(x[1]['chnum'])))
            except:
                scheduleSort = OrderedDict(sorted(iter(schedule.items()), key=lambda x: x[1]['chfcc']))
            for station in scheduleSort:
                fh.write('\t<channel id=\"' + station + '.zap2epg\">\n')
                if 'chtvh' in scheduleSort[station] and scheduleSort[station]['chtvh'] is not None:
                    xchtvh = convHTML(scheduleSort[station]['chtvh'])
                    fh.write('\t\t<display-name>' + xchtvh + '</display-name>\n')
                if 'chnum' in scheduleSort[station] and 'chfcc' in scheduleSort[station]:
                    xchnum = scheduleSort[station]['chnum']
                    xchfcc = scheduleSort[station]['chfcc']
                    xchnam = scheduleSort[station]['chnam']
                    fh.write('\t\t<display-name>' + xchnum + ' ' + convHTML(xchfcc) + '</display-name>\n')
                    if xchnam != "INDEPENDENT":
                        fh.write('\t\t<display-name>' + convHTML(xchnam) + '</display-name>\n')
                    fh.write('\t\t<display-name>' + convHTML(xchfcc) + '</display-name>\n')
                    fh.write('\t\t<display-name>' + xchnum + '</display-name>\n')
                elif 'chfcc' in scheduleSort[station]:
                    xchnum = scheduleSort[station]['chfcc']
                    fh.write('\t\t<display-name>' + convHTML(xchfcc) + '</display-name>\n')
                elif 'chnum' in scheduleSort[station]:
                    xchnum = scheduleSort[station]['chnum']
                    fh.write('\t\t<display-name>' + xchnum + '</display-name>\n')
                if 'chicon' in scheduleSort[station]:
                    fh.write("\t\t<icon src=\"http:" + scheduleSort[station]['chicon'] + "\" />\n")
                fh.write("\t</channel>\n")
                stationCount += 1
        except Exception as e:
            logging.exception('Exception: printStations')

    def addXDetails(edict):
        """Create enhanced description using intelligent default formatting"""
        try:
            # PRIORITY 1: Use extended series description from downloaded JSON if available
            extended_desc = edict.get('epseriesdesc')
            if extended_desc:
                extended_desc = extended_desc.strip()
            else:
                extended_desc = ''

            # PRIORITY 2: Use basic episode description from TV guide
            guide_desc = edict.get('epdesc')
            if guide_desc:
                guide_desc = guide_desc.strip()
            else:
                guide_desc = ''

            # Choose primary description
            if extended_desc and len(extended_desc) > len(guide_desc):
                base_desc = extended_desc
                logging.debug('Using extended series description for %s', edict.get('epshow', 'Unknown'))
            elif guide_desc:
                base_desc = guide_desc
            else:
                return None

            # Start with base description
            enhanced_parts = [base_desc]

            # Add additional info intelligently
            additional_info = []

            # Add year for movies/shows
            if edict.get('epyear') and str(edict['epyear']) != '0':
                additional_info.append(f"({edict['epyear']})")

            # Add season/episode info for series
            if edict.get('epsn') and edict.get('epen'):
                season_ep = f"S{int(edict['epsn']):02d}E{int(edict['epen']):02d}"
                additional_info.append(season_ep)

            # Add premiere date if available
            if edict.get('epoad') and int(edict['epoad']) > 0:
                try:
                    is_dst = time.daylight and time.localtime().tm_isdst > 0
                    TZoffset = (time.altzone if is_dst else time.timezone)
                    origDate = int(edict['epoad']) + TZoffset
                    premiere_date = datetime.datetime.fromtimestamp(origDate).strftime('%Y-%m-%d')
                    additional_info.append(f"Premiered: {premiere_date}")
                except:
                    pass

            # Add rating if available
            if edict.get('eprating'):
                additional_info.append(f"Rated: {edict['eprating']}")

            # Add flags
            flags = []
            if edict.get('epflag'):
                if 'New' in edict['epflag']:
                    flags.append('NEW')
                if 'Live' in edict['epflag']:
                    flags.append('LIVE')
                if 'Premiere' in edict['epflag']:
                    flags.append('PREMIERE')
                if 'Finale' in edict['epflag']:
                    flags.append('FINALE')

            if edict.get('eptags'):
                if 'CC' in edict['eptags']:
                    flags.append('CC')
                if 'HD' in edict['eptags']:
                    flags.append('HD')

            if flags:
                additional_info.append(' '.join(flags))

            # Combine everything intelligently
            if additional_info:
                info_str = ' | '.join(additional_info)
                enhanced_description = f"{base_desc} â€¢ {info_str}"

                # Only return enhanced if it's meaningfully different
                if enhanced_description != base_desc:
                    return enhanced_description

            return base_desc if base_desc else None

        except Exception as e:
            logging.warning('Error creating enhanced description: %s', str(e))
            # Fallback to basic description with safe handling
            basic_desc = edict.get('epdesc')
            return basic_desc.strip() if basic_desc else None

    def printEpisodes(fh):
        global episodeCount
        episodeCount = 0
        basic_desc_count = 0
        enhanced_desc_count = 0
        missing_desc_count = 0
        missing_desc_programs = {}  # Track missing descriptions to avoid duplicates

        try:
            logging.info('Writing Episodes to xmltv.xml file...')
            if xdesc is True:
                logging.info('Enhanced descriptions enabled (using extended data when available)')
            else:
                logging.info('Using basic descriptions only (from TV guide)')

            for station in schedule:
                lang = 'en'
                sdict = schedule[station]
                for episode in sdict:
                    if not episode.startswith("ch"):
                        try:
                            edict = sdict[episode]
                            if 'epstart' in edict:
                                startTime = convTime(edict['epstart'])
                                is_dst = time.daylight and time.localtime().tm_isdst > 0
                                TZoffset = "%.2d%.2d" %(- (time.altzone if is_dst else time.timezone)/3600, 0)
                                stopTime = convTime(edict['epend'])
                                fh.write('\t<programme start=\"' + startTime + ' ' + TZoffset + '\" stop=\"' + stopTime + ' ' + TZoffset + '\" channel=\"' + station + '.zap2epg' + '\">\n')
                                dd_progid = edict['epid']
                                fh.write('\t\t<episode-num system=\"dd_progid\">' + dd_progid[:-4] + '.' + dd_progid[-4:] + '</episode-num>\n')
                                if edict['epshow'] is not None:
                                    fh.write('\t\t<title lang=\"' + lang + '\">' + convHTML(edict['epshow']) + '</title>\n')
                                if edict['eptitle'] is not None:
                                    showTitle = convHTML(edict['eptitle'])
                                    if stitle:
                                        safeTitle = re.sub('[\\/*?:|]', "_", showTitle)
                                        fh.write('\t\t<sub-title lang=\"' + lang + '\">' + safeTitle + '</sub-title>\n')
                                    else:
                                        fh.write('\t\t<sub-title lang=\"' + lang + '\">' + showTitle + '</sub-title>\n')

                                # SIMPLIFIED: Description logic with extended data preference
                                description_written = False

                                # Try enhanced descriptions first if enabled and we have downloaded extended data
                                if xdesc is True and need_extended_download:
                                    try:
                                        enhanced_desc = addXDetails(edict)
                                        if enhanced_desc and enhanced_desc.strip():
                                            # Check if this is truly enhanced (not just the basic description)
                                            basic_desc = edict.get('epdesc', '').strip()
                                            if (enhanced_desc.strip() != basic_desc or
                                                edict.get('epseriesdesc', '').strip()):  # Has extended data
                                                fh.write('\t\t<desc lang=\"' + lang + '\">' + convHTML(enhanced_desc) + '</desc>\n')
                                                enhanced_desc_count += 1
                                                description_written = True
                                    except Exception as e:
                                        logging.warning('Error generating enhanced description for episode %s: %s', episode, str(e))

                                # Fallback to basic description
                                if not description_written:
                                    if edict.get('epdesc') and str(edict['epdesc']).strip():
                                        fh.write('\t\t<desc lang=\"' + lang + '\">' + convHTML(edict['epdesc']) + '</desc>\n')
                                        basic_desc_count += 1
                                        description_written = True

                                # Track missing descriptions (avoid duplicates)
                                if not description_written:
                                    missing_desc_count += 1
                                    program_key = f"{edict.get('epshow', 'Unknown')} - {edict.get('eptitle', 'None')}"

                                    if program_key in missing_desc_programs:
                                        missing_desc_programs[program_key] += 1
                                    else:
                                        missing_desc_programs[program_key] = 1
                                        # Only log detailed message once per program in debug mode
                                        logging.debug('No description available for: %s', program_key)

                                if edict['eplength'] is not None:
                                    fh.write('\t\t<length units="minutes">' + edict['eplength'] + '</length>\n')
                                if edict['epsn'] is not None and edict['epen'] is not None:
                                    fh.write("\t\t<episode-num system=\"onscreen\">" + 'S' + edict['epsn'].zfill(2) + 'E' + edict['epen'].zfill(2) + "</episode-num>\n")
                                    fh.write("\t\t<episode-num system=\"xmltv_ns\">" + str(int(edict['epsn'])-1) +  "." + str(int(edict['epen'])-1) + ".</episode-num>\n")
                                if edict['epyear'] is not None:
                                    fh.write('\t\t<date>' + edict['epyear'] + '</date>\n')
                                if not episode.startswith("MV"):
                                    if epicon == '1':
                                        if edict['epimage'] is not None and edict['epimage'] != '':
                                            fh.write('\t\t<icon src="https://zap2it.tmsimg.com/assets/' + edict['epimage'] + '.jpg" />\n')
                                        else:
                                            if edict['epthumb'] is not None and edict['epthumb'] != '':
                                                fh.write('\t\t<icon src="https://zap2it.tmsimg.com/assets/' + edict['epthumb'] + '.jpg" />\n')
                                    if epicon == '2':
                                        if edict['epthumb'] is not None and edict['epthumb'] != '':
                                            fh.write('\t\t<icon src="https://zap2it.tmsimg.com/assets/' + edict['epthumb'] + '.jpg" />\n')
                                if episode.startswith("MV"):
                                    if edict['epthumb'] is not None and edict['epthumb'] != '':
                                        fh.write('\t\t<icon src="https://zap2it.tmsimg.com/assets/' + edict['epthumb'] + '.jpg" />\n')
                                if not any(i in ['New', 'Live'] for i in edict['epflag']):
                                    fh.write("\t\t<previously-shown ")
                                    if edict['epoad'] is not None and int(edict['epoad']) > 0:
                                        fh.write("start=\"" + convTime(edict['epoad']) + " " + TZoffset + "\"")
                                    fh.write(" />\n")
                                if edict['eptags'] is not None:
                                    if 'CC' in edict['eptags']:
                                        fh.write('\t\t<subtitles type="teletext" />\n')
                                if edict['epflag'] is not None:
                                    if 'Finale' in edict['epflag']:
                                        fh.write("\t\t<last-chance />\n")
                                    if 'Live' in edict['epflag']:
                                        fh.write("\t\t<live />\n")
                                    if 'New' in edict['epflag']:
                                        fh.write("\t\t<new />\n")
                                    if 'Premiere' in edict['epflag']:
                                        fh.write("\t\t<premiere />\n")
                                if edict['eprating'] is not None:
                                    fh.write('\t\t<rating>\n\t\t\t<value>' + edict['eprating'] + '</value>\n\t\t</rating>\n')
                                if edict['epstar'] is not None:
                                    fh.write('\t\t<star-rating>\n\t\t\t<value>' + edict['epstar'] + '/4</value>\n\t\t</star-rating>\n')
                                if edict['epcredits'] is not None:
                                    fh.write("\t\t<credits>\n")
                                    for c in edict['epcredits']:
                                        if c['assetId'] is not None and c['assetId'] != '':
                                            crole=c['role']
                                            fh.write('\t\t\t<' + c['role'].lower() + ' role="' + convHTML(c['characterName']) + '" src="https://zap2it.tmsimg.com/assets/' + c['assetId'] + '.jpg">' + convHTML(c['name']) + '</' + c['role'].lower() + '>\n')
                                        else:
                                            fh.write('\t\t\t<' + c['role'].lower() + ' role="' + convHTML(c['characterName']) + '">' + convHTML(c['name']) + '</' + c['role'].lower() + '>\n')
                                    fh.write("\t\t</credits>\n")
                                if epgenre != '0':
                                    if edict['epfilter'] is not None and edict['epgenres'] is not None:
                                        genreNewList = genreSort(edict['epfilter'], edict['epgenres'])
                                    elif edict['epfilter'] is not None:
                                        genreNewList = edict['epfilter']
                                    if genreNewList != "":
                                        for genre in genreNewList:
                                            fh.write("\t\t<category lang=\"" + lang + "\">" + convHTML(genre).replace('filter-','') + "</category>\n")
                                fh.write("\t</programme>\n")
                                episodeCount += 1
                        except Exception as e:
                            logging.exception('Error processing episode %s:', episode)

            # Log statistics with improved missing description reporting
            logging.info('Description statistics: Episodes=%d, Basic_desc=%d, Enhanced_desc=%d',
                        episodeCount, basic_desc_count, enhanced_desc_count)

            # Report missing descriptions intelligently
            if missing_desc_count > 0:
                logging.info('Missing descriptions: %d episodes from %d unique programs',
                           missing_desc_count, len(missing_desc_programs))

                # In debug mode, show programs with multiple missing descriptions
                if logging.getLogger().isEnabledFor(logging.DEBUG):
                    repeated_programs = [(prog, count) for prog, count in missing_desc_programs.items() if count > 1]
                    if repeated_programs:
                        logging.debug('Programs with multiple missing descriptions:')
                        for prog, count in sorted(repeated_programs, key=lambda x: x[1], reverse=True)[:10]:
                            logging.debug('  %s (x%d)', prog, count)

        except Exception as e:
            logging.exception('Exception: printEpisodes')

    def xmltv():
        try:
            enc = 'utf-8'
            outFile = os.path.join(userdata, os.environ.get('XMLTV'))
            fh = codecs.open(outFile, 'w+b', encoding=enc)
            printHeader(fh, enc)
            printStations(fh)
            printEpisodes(fh)
            printFooter(fh)
            fh.close()
        except Exception as e:
            logging.exception('Exception: xmltv')

    def parseStations(content):
        try:
            ch_guide = json.loads(content)
            for station in ch_guide['channels']:
                skey = station.get('channelId')

                # NOUVELLE LOGIQUE: DÃ©terminer si cette station doit Ãªtre traitÃ©e
                should_process_station = False

                if stationList is not None and len(stationList) > 0:
                    # Utiliser la liste explicite de stations
                    should_process_station = skey in stationList
                    logging.debug('Using explicit station list: %s in list = %s', skey, should_process_station)
                elif tvhmatch and len(tvhMatchDict) > 0:
                    # Utiliser les chaÃ®nes Tvheadend comme filtre
                    chNum = station.get('channelNo', '')
                    chSign = station.get('callSign', '')

                    # VÃ©rifier si le numÃ©ro de chaÃ®ne correspond Ã  une chaÃ®ne Tvheadend
                    # Tester diffÃ©rents formats possibles
                    possible_numbers = [chNum]
                    if '.' not in chNum and chmatch and chSign:
                        chsub = re.search(r'(\d+)$', chSign)
                        if chsub is not None:
                            possible_numbers.append(chNum + '.' + chsub.group(0))
                        else:
                            possible_numbers.append(chNum + '.1')

                    for possible_num in possible_numbers:
                        if possible_num in tvhMatchDict:
                            should_process_station = True
                            logging.debug('Station %s (channel %s->%s) matches Tvheadend channel %s (%s)',
                                        skey, chNum, possible_num, possible_num, tvhMatchDict[possible_num])
                            break

                    if not should_process_station:
                        logging.debug('Station %s (channel %s, callsign %s) not found in Tvheadend - skipping', skey, chNum, chSign)
                else:
                    # Pas de filtrage - prendre toutes les stations
                    should_process_station = True

                if should_process_station:
                    schedule[skey] = {}
                    chSign = station.get('callSign')
                    chName = station.get('affiliateName')
                    schedule[skey]['chfcc'] = chSign
                    schedule[skey]['chnam'] = chName
                    schedule[skey]['chicon'] = station.get('thumbnail').split('?')[0]
                    chnumStart = station.get('channelNo')
                    if '.' not in chnumStart and chmatch and chSign is not None:
                        chsub = re.search(r'(\d+)$', chSign)
                        if chsub is not None:
                            chnumUpdate = chnumStart + '.' + chsub.group(0)
                        else:
                            chnumUpdate = chnumStart + '.1'
                    else:
                        chnumUpdate = chnumStart
                    schedule[skey]['chnum'] = chnumUpdate
                    if tvhmatch and chnumUpdate in tvhMatchDict:
                        schedule[skey]['chtvh'] = tvhMatchDict[chnumUpdate]
                    else:
                        schedule[skey]['chtvh'] = None
        except Exception as e:
            logging.exception('Exception: parseStations')

    def parseEpisodes(content):
        CheckTBA = "Safe"
        try:
            ch_guide = json.loads(content)
            for station in ch_guide['channels']:
                skey = station.get('channelId')

                # MÃŠME LOGIQUE que dans parseStations
                should_process_station = False

                if stationList is not None and len(stationList) > 0:
                    should_process_station = skey in stationList
                elif tvhmatch and len(tvhMatchDict) > 0:
                    chNum = station.get('channelNo', '')
                    chSign = station.get('callSign', '')

                    possible_numbers = [chNum]
                    if '.' not in chNum and chmatch and chSign:
                        chsub = re.search(r'(\d+)$', chSign)
                        if chsub is not None:
                            possible_numbers.append(chNum + '.' + chsub.group(0))
                        else:
                            possible_numbers.append(chNum + '.1')

                    for possible_num in possible_numbers:
                        if possible_num in tvhMatchDict:
                            should_process_station = True
                            break
                else:
                    should_process_station = True

                if should_process_station:
                    episodes = station.get('events')
                    if episodes:  # VÃ©rifier que episodes n'est pas None
                        for episode in episodes:
                            epkey = str(calendar.timegm(time.strptime(episode.get('startTime'), '%Y-%m-%dT%H:%M:%SZ')))
                            schedule[skey][epkey] = {}

                            # Improved description retrieval
                            program = episode.get('program', {})
                            shortDesc = program.get('shortDesc') or ''
                            longDesc = program.get('longDesc') or ''

                            # Handle None values properly
                            if shortDesc is None:
                                shortDesc = ''
                            if longDesc is None:
                                longDesc = ''

                            # Debug: log if no description found
                            if not shortDesc and not longDesc:
                                logging.debug('No description found for: %s - %s',
                                            program.get('title', 'Unknown'),
                                            program.get('episodeTitle', ''))

                            schedule[skey][epkey]['epid'] = episode['program'].get('tmsId')
                            schedule[skey][epkey]['epstart'] = str(calendar.timegm(time.strptime(episode.get('startTime'), '%Y-%m-%dT%H:%M:%SZ')))
                            schedule[skey][epkey]['epend'] = str(calendar.timegm(time.strptime(episode.get('endTime'), '%Y-%m-%dT%H:%M:%SZ')))
                            schedule[skey][epkey]['eplength'] = episode.get('duration')
                            schedule[skey][epkey]['epshow'] = episode['program'].get('title')
                            schedule[skey][epkey]['eptitle'] = episode['program'].get('episodeTitle')
                            schedule[skey][epkey]['epdesc'] = longDesc if longDesc else shortDesc  # Priority to longDesc
                            schedule[skey][epkey]['epyear'] = episode['program'].get('releaseYear')
                            schedule[skey][epkey]['eprating'] = episode.get('rating')
                            schedule[skey][epkey]['epflag'] = episode.get('flag')
                            schedule[skey][epkey]['eptags'] = episode.get('tags')
                            schedule[skey][epkey]['epsn'] = episode['program'].get('season')
                            schedule[skey][epkey]['epen'] = episode['program'].get('episode')
                            schedule[skey][epkey]['epthumb'] = episode.get('thumbnail')
                            schedule[skey][epkey]['epoad'] = None
                            schedule[skey][epkey]['epstar'] = None
                            schedule[skey][epkey]['epfilter'] = episode.get('filter')
                            schedule[skey][epkey]['epgenres'] = None
                            schedule[skey][epkey]['epcredits'] = None
                            schedule[skey][epkey]['epseries'] = episode['program'].get('seriesId')  # seriesId is in program
                            schedule[skey][epkey]['epimage'] = None
                            schedule[skey][epkey]['epfan'] = None
                            schedule[skey][epkey]['epseriesdesc'] = None  # Will be populated by parseXdetails
                            if "TBA" in schedule[skey][epkey]['epshow']:
                                CheckTBA = "Unsafe"
                            elif schedule[skey][epkey]['eptitle']:
                                if "TBA" in schedule[skey][epkey]['eptitle']:
                                    CheckTBA = "Unsafe"
        except Exception as e:
            logging.exception('Exception: parseEpisodes')
        return CheckTBA

    def genShowList():
        showList = []
        for station in schedule:
            sdict = schedule[station]
            for episode in sdict:
                if not episode.startswith("ch"):
                    edict = sdict[episode]
                    if edict.get('epseries'):
                        showList.append(edict['epseries'])
        return showList

    def parseXdetails():
        """Download and parse extended program details"""
        showList = []
        failList = []
        download_count = 0
        success_count = 0

        logging.info('Starting extended details download using optimized session')

        try:
            for station in schedule:
                sdict = schedule[station]
                for episode in sdict:
                    if not episode.startswith("ch"):
                        edict = sdict[episode]
                        EPseries = edict.get('epseries')

                        # Check that EPseries is not None or empty
                        if not EPseries or EPseries in failList:
                            continue

                        showList.append(EPseries)
                        filename = EPseries + '.json'
                        fileDir = os.path.join(cacheDir, filename)

                        try:
                            if not os.path.exists(fileDir):
                                download_count += 1

                                url = 'https://tvlistings.gracenote.com/api/program/overviewDetails'
                                data = 'programSeriesID=' + EPseries

                                # Display complete URL with parameters for debug
                                full_url_display = f'{url}?{data}'
                                logging.info('Downloading extended details for: %s', EPseries)
                                logging.debug('  URL: %s', full_url_display)

                                # Encode data like in the original version
                                data_encode = data.encode('utf-8')

                                # Use optimized downloader with correct parameters
                                content = downloader.download_with_retry_urllib(url, data=data_encode, timeout=6)

                                if content:
                                    try:
                                        # Check that it's valid JSON
                                        json.loads(content)
                                        # Save file
                                        with open(fileDir, "wb") as f:
                                            f.write(content)
                                        logging.info('  Successfully downloaded: %s (%d bytes)', filename, len(content))
                                        success_count += 1
                                    except json.JSONDecodeError:
                                        logging.warning('  Invalid JSON received for: %s', EPseries)
                                        failList.append(EPseries)
                                    except Exception as e:
                                        logging.warning('  Error saving details %s: %s', filename, str(e))
                                        failList.append(EPseries)
                                else:
                                    logging.warning('  Failed to download details for: %s', EPseries)
                                    failList.append(EPseries)

                            # Parse downloaded or cached file
                            if os.path.exists(fileDir):
                                fileSize = os.path.getsize(fileDir)
                                if fileSize > 0:
                                    try:
                                        with open(fileDir, 'rb') as f:
                                            EPdetails = json.loads(f.read())

                                        logging.debug('Parsing %s (%d bytes)', filename, fileSize)

                                        # ENHANCED: Extract extended series description
                                        series_desc = EPdetails.get('seriesDescription')
                                        if series_desc and series_desc.strip():
                                            edict['epseriesdesc'] = series_desc.strip()
                                            logging.debug('Found extended series description for %s: %s',
                                                        EPseries, series_desc[:50] + '...' if len(series_desc) > 50 else series_desc)

                                        # Process other details (existing code)
                                        edict['epimage'] = EPdetails.get('seriesImage')
                                        edict['epfan'] = EPdetails.get('backgroundImage')
                                        EPgenres = EPdetails.get('seriesGenres')

                                        if filename.startswith("MV"):
                                            overview_tab = EPdetails.get('overviewTab', {})
                                            if isinstance(overview_tab, dict):
                                                edict['epcredits'] = overview_tab.get('cast')
                                            EPgenres = 'Movie|' + EPgenres if EPgenres else 'Movie'

                                        if EPgenres:
                                            edict['epgenres'] = EPgenres.split('|')

                                        EPlist = EPdetails.get('upcomingEpisodeTab', [])
                                        if not isinstance(EPlist, list):
                                            EPlist = []

                                        EPid = edict.get('epid', '')
                                        for airing in EPlist:
                                            if not isinstance(airing, dict):
                                                continue

                                            if EPid.lower() == airing.get('tmsID', '').lower():
                                                if not episode.startswith("MV"):
                                                    try:
                                                        origDate = airing.get('originalAirDate')
                                                        if origDate and origDate != '':
                                                            EPoad = re.sub('Z', ':00Z', origDate)
                                                            edict['epoad'] = str(calendar.timegm(time.strptime(EPoad, '%Y-%m-%dT%H:%M:%SZ')))
                                                    except Exception:
                                                        pass # Ignore date parsing errors

                                                    try:
                                                        TBAcheck = airing.get('episodeTitle', '')
                                                        if TBAcheck and "TBA" in TBAcheck:
                                                            try:
                                                                os.remove(fileDir)
                                                                logging.info('  Deleted %s (TBA listing)', filename)
                                                                if EPseries in showList:
                                                                    showList.remove(EPseries)
                                                            except OSError:
                                                                pass
                                                    except Exception:
                                                        pass

                                    except json.JSONDecodeError:
                                        logging.warning('  Invalid JSON in cached file: %s - deleting', filename)
                                        try:
                                            os.remove(fileDir)
                                        except:
                                            pass
                                    except Exception as e:
                                        logging.warning('  Error parsing cached file %s: %s', filename, str(e))

                                else:
                                    logging.warning('  Empty cached file: %s - deleting', filename)
                                    try:
                                        os.remove(fileDir)
                                    except:
                                        pass

                        except Exception as e:
                            logging.warning('  Critical error processing %s: %s', episode, str(e))

            # Final statistics
            stats = downloader.get_stats()
            logging.info('Extended details download completed:')
            logging.info('  Downloads attempted: %d', download_count)
            logging.info('  Successful downloads: %d', success_count)
            logging.info('  Failed downloads: %d', len(failList))
            logging.info('  WAF blocks during details: %d', stats['waf_blocks'])
            logging.info('  Success rate: %.1f%%', (success_count/download_count*100) if download_count > 0 else 0)

            if failList:
                logging.info('  Failed series (first 10): %s', ', '.join(failList[:10]))

        except Exception as e:
            logging.error('Critical error in parseXdetails: %s', str(e))

    try:
        if not os.path.exists(cacheDir):
            os.mkdir(cacheDir)
        count = 0
        gridtime = gridtimeStart
        if stationList is None:
            logging.info('No explicit channel list found in configuration')
        if tvhoff and tvhmatch:
            tvhMatchGet()

        # CORRECTION: Ajouter des logs pour debug aprÃ¨s tvhMatchGet
        if tvhmatch and len(tvhMatchDict) > 0:
            logging.info('Tvheadend channel filtering enabled: %d channels will be used as filter', len(tvhMatchDict))
            logging.debug('Tvheadend channels: %s', list(tvhMatchDict.keys())[:10])  # Show first 10 for debug
        elif stationList and len(stationList) > 0:
            logging.info('Explicit station list filtering: %d stations configured', len(stationList))
        else:
            logging.info('No channel filtering - all available stations will be processed')

        deleteOldCache(gridtimeStart)

        # OPTIMIZED MAIN GUIDE DOWNLOAD
        logging.info('Starting main guide download with optimized session')
        guide_download_start = time.time()

        while count < dayHours:
            filename = str(int(gridtime)) + '.json.gz'
            fileDir = os.path.join(cacheDir, filename)

            if not os.path.exists(fileDir):
                # Build download URL
                url = ('http://tvlistings.gracenote.com/api/grid?aid=orbebb&TMSID=&AffiliateID=lat&FromPage=TV%20Grid&lineupId=&timespan=3&headendId=' +
                       str(lineupcode) + '&country=' + str(country) + '&device=' + str(device) + '&postalCode=' +
                       str(zipcode) + '&time=' + str(int(gridtime)) + '&isOverride=true&pref=-&userId=-')

                logging.info('Downloading guide data for: %i (%s)', int(gridtime), filename)
                logging.debug('  URL: %s', url)

                # Use optimized downloader
                content = downloader.download_with_retry(url, method='GET', timeout=8)  # Shorter timeout for guide

                if content:
                    try:
                        # Check that it's valid JSON
                        json.loads(content)
                        # Save in gzip format
                        savepage(filename, content)
                        logging.info('  Successfully downloaded guide: %s (%d bytes)', filename, len(content))
                    except json.JSONDecodeError:
                        logging.warning('  Invalid JSON received for guide time: %i', int(gridtime))
                    except Exception as e:
                        logging.warning('  Error saving guide %s: %s', filename, str(e))
                else:
                    logging.warning('  Failed to download guide data for: %i', int(gridtime))
            else:
                logging.debug('Using cached guide: %s', filename)

            # Parse downloaded or cached file
            if os.path.exists(fileDir):
                try:
                    with gzip.open(fileDir, 'rb') as f:
                        content = f.read()
                        f.close()
                    logging.info('Parsing %s', filename)
                    if count == 0:
                        parseStations(content)
                    TBAcheck = parseEpisodes(content)
                    if TBAcheck == "Unsafe":
                        try:
                            os.remove(fileDir)
                            logging.info('Deleting %s due to TBA listings', filename)
                        except OSError as e:
                            logging.warning('Error Deleting: %s - %s.' % (e.filename, e.strerror))
                except Exception as e:
                    logging.warning('JSON file error for: %s - deleting file', filename)
                    try:
                        os.remove(fileDir)
                    except:
                        pass
            count += 1
            gridtime = gridtime + 10800

        guide_download_time = time.time() - guide_download_start
        stats = downloader.get_stats()
        logging.info('Main guide download completed in %.1fs', guide_download_time)
        logging.info('  Guide requests: %d, WAF blocks: %d', count, stats['waf_blocks'])

        showList = genShowList()
        deleteOldShowCache(showList)

        # Download extended details if needed (either xdetails=true OR xdesc=true)
        if need_extended_download:
            logging.info('Extended details download enabled (xdetails=%s, xdesc=%s)', xdetails, xdesc)
            parseXdetails()
        else:
            logging.info('Extended details disabled - using basic program information only')
        xmltv()
        timeRun = round((time.time() - pythonStartTime),2)
        logging.info('zap2epg completed in %s seconds. ', timeRun)
        logging.info('%s Stations and %s Episodes written to xmltv.xml file.', str(stationCount), str(episodeCount))

        # Final downloader statistics
        final_stats = downloader.get_stats()
        logging.info('Final download statistics:')
        logging.info('  Total requests: %d', final_stats['total_requests'])
        logging.info('  WAF blocks encountered: %d', final_stats['waf_blocks'])
        logging.info('  Final delay: %.2fs', final_stats['current_delay'])

        return timeRun, stationCount, episodeCount
    except Exception as e:
        logging.exception('Exception: main')
    finally:
        # Clean downloader shutdown
        downloader.close()

if __name__ == '__main__':
    userdata = os.getcwd()
    log = os.path.join(userdata, os.environ.get('LogFile'))

    # Set logging level based on DEBUG environment variable
    debug_enabled = os.environ.get('Debug', 'false').lower() == 'true'
    log_level = logging.DEBUG if debug_enabled else logging.INFO

    logging.basicConfig(filename=log, filemode='w', format='%(asctime)s %(message)s',
                       datefmt='%Y/%m/%d %H:%M:%S', level=log_level)

    if debug_enabled:
        logging.info('Debug logging enabled - verbose output will be shown')

    result = mainRun(userdata)
    if result is None or result == (None, None, None):
        logging.error('Script failed - exiting with error code')
        sys.exit(1)
    else:
        logging.info('Script completed successfully')
        sys.exit(0)
----EOF

python3 -c "$script"
}

###
### MAIN
###

# Create working directories
[ ! -d $CacheDir ] && mkdir -m 755 -p $CacheDir
[ ! -d $LogDir ] && mkdir -m 755 -p $LogDir
[ ! -d $ConfDir ] && mkdir -m 755 -p $ConfDir

# if configuration file doesn't exist
# create one by default (cleaned version without desc01-desc20)
if [ ! -f "$ConfFile" ]; then
read -r -d '' config <<-"----EOF"
<settings version="3">
  <setting id="zipcode">92101</setting>
  <setting id="lineupcode">lineupId</setting>
  <setting id="lineup">Local Over the Air Broadcast</setting>
  <setting id="device">-</setting>
  <setting id="days">1</setting>
  <setting id="redays">1</setting>
  <setting id="slist"></setting>
  <setting id="stitle">false</setting>
  <setting id="xdetails">true</setting>
  <setting id="xdesc">true</setting>
  <setting id="epgenre">3</setting>
  <setting id="epicon">1</setting>
  <setting id="tvhoff">true</setting>
  <setting id="usern"></setting>
  <setting id="passw"></setting>
  <setting id="tvhurl">127.0.0.1</setting>
  <setting id="tvhport">9981</setting>
  <setting id="tvhmatch">true</setting>
  <setting id="chmatch">true</setting>
</settings>
----EOF
echo "${config}" > "$ConfFile"
fi

# Check output path and ability to write
if [[ ! -d $(dirname $XMLTV) ]]; then
   echo "Output directory for [$(basename $XMLTV)] is inaccessible: $(dirname $XMLTV)"
   exit 1
elif [[ ! -w $(dirname $XMLTV) ]]; then
   echo "Output directory for [$(basename $XMLTV)] is not write accessible: $(dirname $XMLTV)"
   exit 1
fi

# Check that config file exists
if [ ! -f $ConfFile ]; then
   echo "Configuration file missing: $ConfFile"
   exit 1
fi

# Check that offset parameter is either empty or match regex
if [ "$Offset" ] && [[ ! "$Offset" =~ $DaysPattern ]]; then
   echo "Parameter [--offset] unmatched: $Offset"
   exit 1
fi

# Check that days parameter is either empty or match regex
if [ "$Days" ] && [[ ! "$Days" =~ $DaysPattern ]]; then
   echo "Parameter [--days] unmatched: $Days"
   exit 1
fi

# Check that Zip & Postal codes are either empty or match regex
if [ "$ZipCode" ] && [[ ! "$ZipCode" =~ $CACodePattern && ! "$ZipCode" =~ $USCodePattern ]]; then
   echo "Parameter [--zip|--postal|--code] unmatched: $ZipCode"
   exit 1
fi

# Make needed variable accessible to python
export CacheDir
export ConfFile
export LogFile
export XMLTV
export Debug
export VERSION
[ "$Days" ] && export Days
[ "$Offset" ] && export Offset
[ "$ZipCode" ] && export ZipCode

# Debug: Display environment variables
echo "DEBUG: Environment variables:" >&2
echo "  CacheDir=$CacheDir" >&2
echo "  ConfFile=$ConfFile" >&2
echo "  LogFile=$LogFile" >&2
echo "  ZipCode=$ZipCode" >&2
echo "  Days=$Days" >&2

# Optional: Clear cache to force new download
if [ "$1" = "--clear-cache" ]; then
    echo "Clearing cache directory..." >&2
    rm -f "$CacheDir"/*.json.gz
    rm -f "$CacheDir"/*.json
    shift
fi

# Generate XMLTV file
zap2epg
result=$?
if [ $result = 0 ]; then
  [ ! "$Quiet" == "true" ] && cat $XMLTV
else
  echo "ERROR: zap2epg script failed with exit code $result" >&2
  echo "Check log file: $LogFile" >&2
  exit 1
fi

exit 0
